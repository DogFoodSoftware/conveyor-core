<?php
   global $extraHeader;
   $extraHeader = <<<EOT
<script type="text/javascript" src="http://d3js.org/d3.v3.min.js"></script>
    <style type="text/css">
/* style notes:
   - would like to better disinguish 'artifact' from 'human tasks'; shapes?
     (Did try to knock this out, isn't trivially straightforward though the
     basic idea is clear enough.)
   - would like way to denote 'scheduled' items triggered regularly by
     calender / time; different circle border?
   - need to review all colors; I like the links OK, but the nodes have no
     thought
   - indicate semi- or possibly automated items?
   - right now, we don't show automated nodes... may want to in larger diagram
*/
path.link {
  fill: none;
  stroke: #666;
  stroke-width: 1.5px;
}

marker#live-sync {
  fill: #0cc;
}

path.link.live-sync {
  stroke: #0cc;
  stroke-dasharray: 5, 1;
}

marker#has-a {
  fill: #cc0;
}

path.link.has-a {
  stroke: #cc0;
  stroke-dasharray: 0,2 1;
}

circle {
  fill: #ccc;
  stroke: #333;
  stroke-width: 1.5px;
}

circle.physical-host {
  fill: #d00;
}

circle.drive {
  fill: #0d0;
}

circle.ext4-volume {
  fill: #00d;
}

circle.lvm-volume {
  fill: #88f;
}

circle.virtual-host {
  fill: #262;
}

/*
circle.operations {
  fill: #a22;
}

circle.externality {
  fill: #000;
}

circle.support {
  fill: #f82;
}

circle.sales {
  fill: #ff0;
}

circle.artifact {
  fill: #c0c;
}
*/

text {
  font: 10px sans-serif;
  pointer-events: none;
}

text.shadow {
  stroke: #fff;
  stroke-width: 3px;
  stroke-opacity: .8;
}

    </style>
EOT;
?>
<div id="viz" class="grid_12"></div>
    <script type="text/javascript">

/**
 * <div class="p">
 * Basic idea is you start with tasks that generate plans. The plans are
 * themselves expressed as tasks. Completion of a task may generate follow on
 * tasks. Some tasks are triggered periodically, some tasks are event
 * driven. 
 * </div>
 * <div id="" class="subHeader">Working Spec<span>
 * <div class="p">
 * The spec is still a work in process. At the moment, we're going with the
 * following:
 * <ol>
 *   <li>There are three fundamental node types: 'activities', 'resources',
 *   and 'events'.</li>
 *   <li>The fundamental node types are further decomposed into one and only
 *   one of the following contrete types:
 *     <ol>
 *       <li>Activities are organized by the <href="TODO">department</a>:
 *         <ol>
 *           <li>'<a href="TODO">vision</a>',</li>
 *           <li>'<a href="TODO">management</a>',</li>
 *           <li>'<a href="TODO">implementation</a>',</li>
 *           <li>'<a href="TODO">operations</a>',</li>
 *           <li>'<a href="TODO">sales</a>',</li>
 *           <li>'<a href="TODO">support</a>', and</li>
 *           <li>'<a href="TODO">finance</a>'.</li>
 *         </ol></li>
 *       <li>Resources are one of the <a href="TODO">Conveyor basic
 *       resources<a>. (Will list and link after we develop the map a bit
 *       further.)</li>
 *       <li>Events are either 'interupts' or 'scheduled'.</li>
 *     </ol></li>
 *   <li>An update or triggers emanating from a resource represents automated
 *   action by Conveyor logic.</li>
 *   <li>There are three fundamental node interactions (captured as node
 *   paths): 'trigger', 'update', and 'informs'.</li>
 *   <li>The following interaction list is exhaustive of all valid interactions:
 *     <ol>
 *       <li>An actor may be triggered by another actor or resource.</li>
 *       <li>A resource may be updated by an actor or another resource</li>
 *       <li>An actor may be informed by a resource.</li>
 *     </ol></li>
 * </ol>
 * </div>
*/

var links = [
    /* physical back up map */
    {source: "VM host", interaction: "has-a", target: "physical root drive"},
    {source: "VM host", interaction: "has-a", target: "physical root drive mirror"},
    {source: "physical root drive", interaction: "live-sync", target: "physical root drive mirror"},
    {source: "physical root drive mirror", interaction: "live-sync", target: "physical root drive"},
];

var nodes = {
    /* physical hosts, drives, and volumes */
    "VM host": { name: "VM host", type: "physical-host" },
    "physical root drive": { name: "physical root drive", type: "drive" },
    "physical root drive mirror": { name: "physical root drive mirror", type: "drive" },
    "root volume - /": { name: "root volume - /", type: "ext4-volume" },
    "log volume - /var": { name: "log volume - /var", type: "lvm-volume" },
    "home volume - /home": { name: "home volume - /home", type: "lvm-volume"},

    "Mac workstation" : { name: "Mac workstation", type: "physical-host" },
    "Mac drive": { name: "Mac drive", type: "drive" },
    "Mac Time Machine drive": { name: "Mac Time Machine drive", type: "drive"},
    /* virtual hosts */
    "production host": { name: "production server", type: "virtual-host" },
    "stateless host": { name: "stateless host", type: "virtual-host" },
    "virtual workstation": { name: "virtual workstation", type: "virtual-host" }
};

// Typos in the node names will result in a creation of grey links which can
// be visually identified and corrected.

// Compute the distinct nodes from the links.
links.forEach(function(link) {
  link.source = nodes[link.source] || (nodes[link.source] = {name: link.source});
  link.target = nodes[link.target] || (nodes[link.target] = {name: link.target});
});

var w = 960,
    h = 500;

var force = d3.layout.force()
    .nodes(d3.values(nodes))
    .links(links)
    .size([w, h])
    .linkDistance(60)
    .charge(-300)
    .on("tick", tick)
    .start();

var svg = d3.select("#viz").append("svg:svg")
    .attr("width", w)
    .attr("height", h);

// Per-type markers, as they don't inherit styles.
svg.append("svg:defs").selectAll("marker")
    .data(["has-a", "live-sync"])
  .enter().append("svg:marker")
    .attr("id", String)
    .attr("viewBox", "0 -5 10 10")
    .attr("refX", 15)
    .attr("refY", -1.5)
    .attr("markerWidth", 6)
    .attr("markerHeight", 6)
    .attr("orient", "auto")
  .append("svg:path")
    .attr("d", "M0,-5L10,0L0,5");

var path = svg.append("svg:g").selectAll("path")
    .data(force.links())
  .enter().append("svg:path")
    .attr("class", function(d) { return "link " + d.interaction; })
    .attr("marker-end", function(d) { return "url(#" + d.interaction + ")"; });

var node = svg.append("svg:g").selectAll("circle")
    .data(force.nodes())
  .enter().append("svg:circle")
    /* added */
    .attr("class", function(d) { return d.type })
    .attr("r", 6)
    .call(force.drag);

var text = svg.append("svg:g").selectAll("g")
    .data(force.nodes())
  .enter().append("svg:g");

// A copy of the text with a thick white stroke for legibility.
text.append("svg:text")
    .attr("x", 8)
    .attr("y", ".31em")
    .attr("class", "shadow")
    .text(function(d) { return d.name; });

text.append("svg:text")
    .attr("x", 8)
    .attr("y", ".31em")
    .text(function(d) { return d.name; });

// Use elliptical arc path segments to doubly-encode directionality.
function tick() {
  path.attr("d", function(d) {
    var dx = d.target.x - d.source.x,
        dy = d.target.y - d.source.y,
        dr = Math.sqrt(dx * dx + dy * dy);
    return "M" + d.source.x + "," + d.source.y + "A" + dr + "," + dr + " 0 0,1 " + d.target.x + "," + d.target.y;
  });

  node.attr("transform", function(d) {
    return "translate(" + d.x + "," + d.y + ")";
  });

  text.attr("transform", function(d) {
    return "translate(" + d.x + "," + d.y + ")";
  });
}

    </script>

<div id="Backup-Protocol" class="blurbSummary grid_12"
     data-perspective="implementation">
  <div class="blurbTitle">Backup Protocol</div>
  <div class="p">
    The backup protocol determine what gets backed up, when, and how. The
    answer to "what gets backed up" is "everything". With storage costs what
    they are, it's easier and cheaper to back up everything than to deal with
    the extra cost of trying to separate out bits and pieces.
  </div>
  <div class="p">
    To answer the 'when' and the 'how', we find it useful to break up the data
    into four groups:
    <ul>
      <li>physical host / distro software,</li>
      <li>physical host <code>/var</code> and <code>/home</code>,</li>
      <li>Conveyor VMs, and</li>
      <li>Conveyor data.</li>
    </ul>
    This breaks the total 'universe of bits' into four categories which
    determine the method and options available. These being:
    <ul>
      <li>re-installation,</li>
      <li>disk level mirroring, and</li>
      <li>periodic snapshots.</li>
    </ul>
  </div>
  <div class="p">
    Of course, other bit-divisions and methods are possible and, in future may
    even be necessary. However, for the current goals, the proposed divisions
    and methods are deems sufficient in terms of functionality and minimal in
    terms of complexity.
  </div>
  <div class="subHeader"><span>Re-Installation</span></div>
  <div class="p">
    Re-installation means the bits are not explicitly backed up. Rather, we
    trust to the original installation as being reliable. Re-installation
    is&mdash;on it's own&mdash;is essentially incompatible with
    customization. It also presents problems where it is necessary to back up
    to a specific version as generally one would re-install and patch, and the
    patches available at the time of install may be newer than the patches
    last applied to the software being restored.
  </div>
  <div class="subHeader"><span>Disk Level Mirroring</span></div>
  <div class="p">
    Disk level mirroring, or RAID 1, creates a live copy of the disk. Under
    certain failure scenarios in which one hard drive detectably fails while
    the other remains operational, the 'mirrored' image remains functionally
    unaffected and the mirror array is able to operate normally, except that
    of course further failure cannot be tolerated.
  </div>
  <div class="p">
    Mirroring increases cost and complexity in order to make downtime more
    predictible. Depending on the hardware configuration and the nature of the
    error, mirroing may or may not increase overall uptime. That is, in the
    face of the clean failure of a disk, the first benefit is that one need
    not deal with the failure immediately because current operations array is
    not immediately effected. The disk still must be replaced (and as there
    are more disks, it's not clear that this creates less work), but the work
    need not be performed immediately and may be scheduled with greater
    flexibility.
  </div>
  <div class="p">
    It is possible to use disk arrays for other purposes, such as improved
    performance and as a tool to create large volumes. Given current
    mainstream technologies and our primary target market we opt
    to <a href="#Keep-It-Simple">keep it simple</a> and just consider the
    single, disk-to-disk mirroring
    scenario. <a href="#Future-Considerations">Future considerations</a> are
    discussed later.
  </div>
  <div id="Periodic-Snapshots" class="subHeader"><span>Periodic Snapshots</span></div>
  <div class="p">
    A periodic snapshot is just what it says. A snapshot of bits at a
    particular point in time. Snapshots may be taken at the volume or image
    level, or in a file-by-file manner.
  </div>
  <div class="p">
    A volume snapshot requires a compatible volume management system which can
    gurantee that all bits recorderd in the backup reflect the state of the
    volume as a whole at a given point in time even if some of the bits are
    changed before they are recorded. In the background, the volume manager
    tracks changes to volume bits in order to allow the original volume to
    continue functioning normally, but still provide a correct answer as to
    "what any given bit was" at the time of the snapshot to the backup.
  </div>
  <div class="p">
    Image snapshots work in a similar manner, but rather than a volume
    manager, the 'snapshot' capability is provided by the virtual machine
    manager, allowing one to take a consistent point-in-time snapshot of a
    running VM.
  </div>
  <div id="Disks-and-Volumes" class="subHeader"><span>Disks and Volumes</span></div>
  <div class="p">
    Disks are really big. At the sweet spot for $/byte for static drives is
    around 128-256GB. For 3.5, it's 3TB. For the majority of profitable web
    apps, that's more data than they need; and for many of them, capacity will
    improve faster than data accumulates.
  </div>
  <div class="p">
    Those kinds of volumes are big enough such that most environments will
    never need to create multi-drive volumes. As far as applications may
    utilize multi-volume data storage, even "big data" applications need not
    necessarily deal with multi-drive volumes, which reduces complexity.
  </div>
  <div class="p">
    The goal is to keep the backup policy simple for those that end up running
    their own physical machines in a production. Once you get into multi-drive
    volumes, you can't do disk-by-disk backup. As the fundamental physical
    unit of storage, the disk is itself the easiest thing to plan backups
    around.
  </div>
  <div class="subHeader"><span>RAID Policy</span></div>
  <div class="p">
    As discussed under <a href="#Disks-and-Volumes">Disks and
      Volumes</a>, there is often no need for multi-drive volumes in many
    environments. Which leaves 'performance' and 'reliability' as possible
    reasons to use RAID.
  </div>
  <div class="p">
    First, WRT to performance, SSDs are pretty fast. In fact, if you really
    need performance,
    it's <a href="http://www.mysqlperformanceblog.com/2010/01/18/fast-storage-8-ssd-intel-x-25m-80gb-benchmarks/">PCI-based
    SSDs you'll
    want</a>. Any RAID system adds significant complexity, and with
    SSDs <a href="http://research.microsoft.com/en-us/um/people/maheshba/papers/hotstorage09-raid.pdf">there's
    even more to worry about</a>. So start simple and move to RAID only if
    necessary. Remember, to benefit from a high performance ceiling, you need
    to be operating under sufficient demand to actually tax the
    system. Network transfer times are going to quast any micro-second
    improvements under unloaded scenarios. Many environments would not exhibit
    any observable benefits from a complex RAID system when compared to a
    single SSD volume.
  </div>
  <div class="p">
    So, what of reliability? Our own exeperience has been that unless actively
    maintained and monitored, RAID arrays are less reliable, due to their
    complexity and the additional failure point of the RAID card. With
    dedicated process and attention, RAID can be more reliable, but it's not a
    given.
  </div>
  <div class="p">
    In short, today's huge single drives and fast SSDs have raised the bar
    such that RAID is simply not necessary in many, if not most
    environments. RAID is not in-and-of-itself a particularly hard thing, but
    it does add yet another layer to the physical infrastructure and that much
    more complexity. We should be glad that in many situations, it's simply
    unnecessary.
  </div>
  <div class="subHeader"><span>Backing Up Physical Hosts; Distro and Logs</span></div>
  <div class="p">
    Backing up a Mac is easy. Use TimeMachine.
  </div>
  <div class="p">
    Dedicated Linux workstations should be laid out with a single root disk,
    partitioned into a straight ext4 root partition of 20GB for standard hosts
    and 10GB for shell-only servers (without a window manager). The remainder
    of the root disk should be as LVM storage generally partitioned into 2
    volumes: 1GB for the <code>/var</code> and the remainder
    for <code>/home</code> directories. Leave 1-2GB unallocated to support LVM
    snapshots. This allows the system logs and data to be backed up via volume
    snapshots. The remainder of the root file system is simply re-installed if
    the volume is lost.
  </div>
  <div class="subHeader"><span>Backing Up VMs</span></div>
  <div class="subHeader"><span>Backing Up Data Volumes</span></div>
</div><!-- .blurbSummary#Backup-Protocol -->
<div id="Host-Upgrades" class="blurbSummary grid_12">
  <div class="blurbTitle">Host Upgrades</div>
  <div class="p">
    Hosts components are normally updated on a regular, scheduled basis and as
    needed in response to hardware failures or data loss. The infrastructure
    is designed to allow this constant update process to proceed with minimal
    service interruptions while keeping things simple. Because it is often
    easier and ultimately more reliable to plan for short offline periods
    rather than attempt 'zero-interruption', operational components, and
    especially <a href="/documenation/kibbles/ref/Web_Services">web
    services</a> should support user notifications / warnings and graceful
    downtime entry.
  </div>
  <div class="subHeader"><span>Hardware Component Upgrades</span></div>
  <div class="p">
    Host component upgrades, such as additional memory or upgraded power
    supplies are handled during planned downtime. Component upgrades are
    generally done in reaction to ad hoc developments. Installers should be
    prepared to respond if the new components indicate fault on reboot and the
    components should be removed. If possible, backup components should be on
    hand. If not, then the host may be returned to the pre-updgrade state.
  </div>
  <div class="subHeader"><span>Hardware Replacement</span></div>
  <div class="p">
    In some ways, hardware replacement is the most straightforward. Full
    replacement is on a 3 (preferred) or 4.5 (standard) year schedule. Hardware
    replacement is generally timed to coincide with OS updates or replacements
    as well. Recall that physical hosts are used exclusively as VM and data
    volume hosts. So, the new host is prepared, and VMs are teleported to the
    new host. Volumes data is copied, or the physical volume media is
    transferred as appropriate. Coordinating the teleport with the data
    transfer may not be possible in all cases, in which some VMs may be
    shutdown and the images copied to the new host. Certain long lived
    components, esp. memory, may be transfered to new hosts. Some hosts may
    also be down-cycled and re-used with new disks.
  </div>
  <div class="subHeader"><span>OS Update</span></div>
  <div class="p">
    OS updates occur 18 months after the last update and with every hardware
    replacement. This means that every other update should occur with hardware
    replacement on the preferred schedlue and with every third update on a
    standard schedule, and that at any given time there are at most 3 OS
    versions (not counting workstation OSes). According to the
    <a href="http://en.wikipedia.org/wiki/SUSE_Linux_distributions#History">openSUSE
      distro release history</a> each upgrade should advance one or two distro
      versions.
  </div>
  <div class="p">
    OS updates must all be checked before released. With hardware replacement,
    this is easy because you've got a new machine not initially used, and so
    it's easy to test it. When updating a physical host in a non-replacement
    cycle or whenever updating a virtual host, it is necessary to clone the
    machine first. Because of this requirement, physical hosts may, when times
    are tough and/or the updside is not considered siginificant, skip
    non-replacement upgrades (because in such situation there may no be desire
    or ability to invest in the necessary capitacl resources in order to
    create a testable clone). With VMs, it is unclear yet whether we prefer to
    update the clone, and then sync data and let the clone replace the
    original, or if the clone is just to test an update which is then run on
    the original. The latter seems the most likely.
  </div>
</div><!-- .blurbSummary#Host-Upgrades -->
